{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "category_columns = [\n",
    "    \"Unlawful detention\",\n",
    "    \"Human trafficking\",\n",
    "    \"Enslavement\",\n",
    "    \"Willful killing of civilians\",\n",
    "    \"Mass execution\",\n",
    "    \"Kidnapping\",\n",
    "    \"Extrajudicial killing\",\n",
    "    \"Forced disappearance\",\n",
    "    \"Damage or destruction of civilian critical infrastructure\",\n",
    "    \"Damage or destruction, looting, or theft of cultural heritage\",\n",
    "    \"Military operations (battle, shelling)\",\n",
    "    \"Gender-based or other conflict-related sexual violence\",\n",
    "    \"Violent crackdowns on protesters/opponents/civil rights abuse\",\n",
    "    \"Indiscriminate use of weapons\",\n",
    "    \"Torture or indications of torture\",\n",
    "    \"Persecution based on political, racial, ethnic, gender, or sexual orientation\",\n",
    "    \"Movement of military, paramilitary, or other troops and equipment\"\n",
    "]\n",
    "\n",
    "# 2) Custom Dataset class for articles\n",
    "class ArticleDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train.csv\")  \n",
    "val_df   = pd.read_csv(\"val.csv\")    \n",
    "test_df  = pd.read_csv(\"test.csv\")  \n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize the text columns\n",
    "train_encodings = tokenizer(\n",
    "    list(train_df[\"Incident Narrative\"].values),\n",
    "    truncation=True,\n",
    "    padding=True\n",
    ")\n",
    "val_encodings   = tokenizer(\n",
    "    list(val_df[\"Incident Narrative\"].values),\n",
    "    truncation=True,\n",
    "    padding=True\n",
    ")\n",
    "test_encodings  = tokenizer(\n",
    "    list(test_df[\"Incident Narrative\"].values),\n",
    "    truncation=True,\n",
    "    padding=True\n",
    ")\n",
    "\n",
    "# Extract labels (multi-label targets in your category columns)\n",
    "train_labels = train_df[category_columns].values\n",
    "val_labels   = val_df[category_columns].values\n",
    "test_labels  = test_df[category_columns].values\n",
    "\n",
    "# Create Dataset objects\n",
    "train_dataset = ArticleDataset(train_encodings, train_labels)\n",
    "val_dataset   = ArticleDataset(val_encodings, val_labels)\n",
    "test_dataset  = ArticleDataset(test_encodings, test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Note: num_labels = number of category columns for multi-label classification\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", \n",
    "    num_labels=len(category_columns)\n",
    ")\n",
    "\n",
    "# Define compute_metrics for multi-label classification\n",
    "def compute_metrics(p):\n",
    "    # p.predictions are logits; p.label_ids are the ground truth\n",
    "    preds = torch.sigmoid(torch.tensor(p.predictions))  # Convert logits to probabilities\n",
    "    preds = (preds > 0.5).int().cpu().numpy() \n",
    "    labels = torch.tensor(p.label_ids).cpu().numpy()\n",
    "    \n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels,\n",
    "        preds,\n",
    "        average='weighted'\n",
    "    )\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # Output directory\n",
    "    eval_strategy=\"epoch\",           # Evaluate at the end of each epoch\n",
    "    save_strategy=\"epoch\",           # Save model at the end of each epoch\n",
    "    learning_rate=2e-5,              # Learning rate\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=30,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",      # Use F1 score for best model\n",
    "    logging_dir='./logs'\n",
    ")\n",
    "\n",
    "# Initialize Trainer with training and validation sets\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model(\"./bert-multiclass-model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_model(dataset, threshold=0.5):\n",
    "    loader = DataLoader(dataset, batch_size=16)\n",
    "    model.eval()  # Set model to eval mode\n",
    "    \n",
    "    preds_list = []\n",
    "    labels_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            inputs = {\n",
    "                key: val.to(model.device) for key, val in batch.items() if key != 'labels'\n",
    "            }\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            # Convert logits to probabilities\n",
    "            probs = torch.sigmoid(logits).cpu().numpy()\n",
    "            # Apply threshold\n",
    "            preds = (probs > threshold).astype(int)\n",
    "            \n",
    "            preds_list.extend(preds)\n",
    "            labels_list.extend(batch['labels'].cpu().numpy())\n",
    "    \n",
    "    preds_array = np.array(preds_list)\n",
    "    labels_array = np.array(labels_list)\n",
    "    \n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels_array, preds_array, average='weighted')\n",
    "    acc = accuracy_score(labels_array, preds_array)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "# Run evaluation on the test dataset\n",
    "test_results = evaluate_model(test_dataset)\n",
    "print(\"Final Test Set Evaluation Results:\", test_results)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
